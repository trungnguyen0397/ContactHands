
<html xmlns="http://www.w3.org/1999/xhtml" class="gr__cs_cmu_edu">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title> ContactHands</title>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
</head>
<body data-gr-c-s-loaded="true">
<table width="1000" ,="" align="center">
    <tbody>
    <tr>
        <br>
        <br>
        <td width="908">
            <h1 align="center"><font size="15" ,="" font="font" face="Arial"><strong> ContactHands </strong></font></h1>
            <br>
            <p align="center"> <font size="5" ,="" font="font" face="Arial">
                 A large-scale dataset of in-the-wild images for hand detection and contact recognition.
        </p>

         <p align="center"> <font size="4" ,="" font="font" face="Arial">
                <a href="https://www3.cs.stonybrook.edu/~sunarasimhas" target="_blank">Supreeth Narasimhaswamy</a><sup>1</sup>, <a href="https://trungnguyen0397.github.io/" target="_blank">Trung Nguyen</a><sup>2</sup>, <a href="https://www3.cs.stonybrook.edu/~minhhoai/index.html" target="_blank">Minh Hoai</a><sup>1,2</sup>
        </font> </p>
              </font> </p>
        <p align="center">
        <font size="4" ,="" font="font" face="Arial"> <sup>1</sup>Stony Brook University, Stony Brook, NY 11790, USA
        <br>
        <sup> 2</sup>VinAI Research, Hanoi, Vietnam
        </font> 
        <br>
        <br>
        <br>

        <p align="center"><img src="./images/data_vis.png" alt="" width="1000" height="420">   </p>
        <p align="center">
        <br>
        <p align="left">
        <p align="justify">
        <font size="4" ,="" font="font" face="Arial"> This work is based on <a href="https://arxiv.org/pdf/2010.09676.pdf" target="_blank">our NeurIPS 2020 paper</a>. We collect a large-scale dataset of unconstrained images and annotate hand locations and their contact states.  
        <br>
        <br>
        We annotate hands by quadrilateral bounding boxes and provide contact state annotations by recognizing the following four conditions, namely: (1) No-Contact: the hand is not in contact with any object in the scene; (2) Self-Contact: the hand is in contact with another body part of the same person; (3) Other-Person-Contact: the hand is in contact with another person; and (4) Object-Contact: the hand is holding or touching an object other than people. These conditions are not mutually exclusive, and a hand can be in multiple states. For each hand instance, we annotate the four contact states by answering Yes, No, or Unsure.
        <br>
        <br>
        Our dataset has annotations for 20,516 images, of which 18,877 form the training set and 1,629 form the test set.
        </font>
        
        <br>
        <br>
        <br>

        <p><font size="5" ,="" font="font" face="Arial"><b>Examples</b></font></p> 
        
        <div align="center">
            <img src="./images/data1-1.png" alt="" width="280" height="250"/> 
            <img src="./images/data2-1.png" alt="" width="280" height="250"/> 
            <img src="./images/data3-1.png" alt="" width="280" height="250"/> 
            <img src="./images/data4-1.png" alt="" width="280" height="250"/>
            <img src="./images/data5-1.png" alt="" width="280" height="250"/>
            <img src="./images/data6-1.png" alt="" width="280" height="250"/>
        </div>
        <br>
        <p align="left">
            <font size="3" ,="" font="font" face="Arial">
                <b>Sample data from ContactHands.</b> We show the bounding box annotations in green color. 
                To avoid clutter, we display contact states for only two hand instances per image. 
                The notations NC, SC, PC, and OC denote No-Contact, Self-Contact, Other-Person-Contact, and Object-Contact. 
                We highlight the contact state for a hand by <a style="color:red">red</a> color. If a contact state is unsure, we highlight it in <a style="color:blue">blue</a>. 
            </font>
        </p>
        <br>
        <!-- <p><font size="5" ,="" font="font" face="Arial"><b>Paper</b></font></p> 
        
        <p>
            <font size="4" ,="" font="font" face="Arial"> <a href="https://arxiv.org/pdf/2010.09676.pdf" target="_blank"> Detecting Hands and Recognizing Physical Contact in the Wild</a>. Supreeth Narasimhaswamy, Trung Nguyen, and M. Hoai, Advances in Neural Information Processing Systems (NeurIPS) 2020.        
        </p>
        
        <br> -->
        <p><font size="5" ,="" font="font" face="Arial"><b>Citation</b></font></p>
        <p align="left">
            <font size="4" ,="" font="font" face="Arial">
                If you find our work useful, please consider citing:
            </font>
        </p>
        <div align="left">
        <pre>
            <code>
    @inproceedings{contacthands_2020,
        title={Detecting Hands and Recognizing Physical Contact in the Wild},
        author={Supreeth Narasimhaswamy and Trung Nguyen and Minh Hoai},
        booktitle={Advances in Neural Information Processing Systems},
        year={2020},
    }
            </code>
        </pre>
        </div>  

        <p align="left">
        <font size="5" ,="" font="font" face="Arial"><b>Download </b></font></p> 
        <p align="justify">
        <font size="4" ,="" font="font" face="Arial"> The dataset (4.5G) can be dowloaded from <a href="https://public.vinai.io/ContactHands.zip" target="_blank"><strong>VinAI Server</strong></a>
             and <a href="https://drive.google.com/file/d/1bhl1ZJjkSr3lTP61CNpj6KqIAPCSYJ3q/view?usp=sharing" target="_blank"><strong>Google Drive</strong></a>.
         <br>
         <br>

        <p align="left">
        <font size="5" ,="" font="font" face="Arial"><b>Copyright and Disclaimer</b></font></p>

        <p align="left">
        <font size="4" ,="" font="font" face="Arial"> 
            <em>
                Copyright: 
            </em>
            Notwithstanding the publish and/or disclosure of this document, the data, and any material in pertaining to this document (collectively the “Works”), all copyright and all rights therein are maintained by and remained proprietary property of the authors and/or the copyright holders. 
        </font></p>
        <p align="left">
            <font size="4" ,="" font="font" face="Arial">
                <em>
                    Disclaimer: 
                </em>
                The Works are provided “AS IS” without warranties or conditions of any kind either expressed or implied.  To the fullest extent possible under applicable law, the authors and/or the copyright holders disclaim any and all warranties and conditions, expressed or implied, including but not limited to, implied warranties or conditions of merchantability and fitness for a particular purpose, non-infringement or other violation of rights or breach of contract.  The authors and/or the copyright holders of the Works do not warrant nor make any representations of any kind in relation to, or in connection with the use, accuracy, timelines, completeness, efficacy, fitness, applicability, performance, security, availability or reliability of the Works, or the results from the use of the Works. In no event and under no circumstance shall the authors and/or the copyright holders of the Works be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising out of or in connection with the Works, or the use of the Works.
            </font></p>
        
        <br>
        <br>
        <br>
        <br>
        <br>
    </tr>
    </tbody>
</table>
</body>
<span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>
